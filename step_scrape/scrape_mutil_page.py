import requests
from bs4 import BeautifulSoup
import csv
import json
import time

def scrape_books_toscrape():
    base_url = "https://books.toscrape.com"
    all_books = []
    page = 1

    while True:
        try:
            if page == 1:
                url = f"{base_url}/index.html"
            else:
                url = f"{base_url}/catalogue/page-{page}.html"
            
            req = requests.get(url)

            if req.status_code != 200:
                print(f"Kh√¥ng th·ªÉ truy c·∫≠p trang {page}")
                break

            soup = BeautifulSoup(req.text, 'html.parser')
            books = soup.find_all('article', class_='product_pod')

            if not books:
                print(f"ƒê√£ scrape xong t·∫•t c·∫£ trang (d·ª´ng ·ªü trang {page-1})")
                break

            # X·ª≠ l√Ω t·ª´ng cu·ªën s√°ch
            for book in books:
                book_data = extract_book_info(book, base_url)
                all_books.append(book_data)
                print(f"{book_data['title']} - ¬£{book_data['price']}")
            
            page += 1
            time.sleep(1)  # Delay ƒë·ªÉ tr√°nh overload server
        
        except Exception as e:
            print(f"‚ùå L·ªói ·ªü trang {page}: {e}")
            break
    
    save_results(all_books)
    return all_books

def extract_book_info(book_element, base_url):
    book_data = {}
    
    try:
        # Ti√™u ƒë·ªÅ
        title_elem = book_element.find('h3').find('a')
        book_data['title'] = title_elem.get('title', '').strip()
        book_data['url'] = base_url + '/catalogue/' + title_elem.get('href', '')
        
        # Gi√°
        price_elem = book_element.find('p', class_='price_color')
        clean_price1 = price_elem.text.strip()
        clean_price2 = clean_price1.replace('√Ç', '').strip()
        book_data['price'] = clean_price2 if price_elem else 'N/A'
        
        # T√¨nh tr·∫°ng stock
        stock_elem = book_element.find('p', class_='instock availability')
        book_data['stock'] = stock_elem.text.strip() if stock_elem else 'N/A'
        
        # Rating (s·ªë sao)
        rating_elem = book_element.find('p', class_='star-rating')
        if rating_elem:
            rating_class = rating_elem.get('class', [])
            rating = [cls for cls in rating_class if cls != 'star-rating']
            book_data['rating'] = rating[0] if rating else 'N/A'
        else:
            book_data['rating'] = 'N/A'
        
        # H√¨nh ·∫£nh
        img_elem = book_element.find('img')
        if img_elem:
            book_data['image_url'] = base_url + img_elem.get('src', '').replace('../', '')
        else:
            book_data['image_url'] = 'N/A'
            
    except Exception as e:
        print(f"L·ªói khi x·ª≠ l√Ω s√°ch: {e}")
        book_data = {
            'title': 'Error',
            'price': 'N/A',
            'stock': 'N/A',
            'rating': 'N/A',
            'url': 'N/A',
            'image_url': 'N/A'
        }
    
    return book_data

def save_results(books):

    # L∆∞u CSV
    try:
        with open('books_data.csv', 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['title', 'price', 'stock', 'rating', 'url', 'image_url']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            writer.writeheader()
            for book in books:
                writer.writerow(book)     
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u CSV: {e}")
    
    # L∆∞u JSON
    try:
        with open('books_data.json', 'w', encoding='utf-8') as jsonfile:
            json.dump(books, jsonfile, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u JSON: {e}")


#Scrape s√°ch theo t·ª´ng category
def scrape_by_category():
    base_url = "https://books.toscrape.com"
    
    # L·∫•y trang ch·ªß ƒë·ªÉ t√¨m categories
    response = requests.get(base_url)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # T√¨m sidebar categories
    sidebar = soup.find('div', class_='side_categories')
    if sidebar:
        categories = sidebar.find_all('a')
        
        for category in categories:
            category_name = category.text.strip()
            category_url = base_url + '/' + category.get('href', '')
            
            print(f"Category: {category_name}")
            print(f"üîó URL: {category_url}")
            
            # Scrape category n√†y
            scrape_category(category_url, category_name)

def scrape_category(category_url, category_name):
    all_books = []
    page = 1

    while True:
        try:
            if page == 1:
                url = category_url
            else:
                url = category_url.replace('/index.html', f'/page-{page}.html')
            
            print(f"Trang {page}: {url}")
            response = requests.get(url)
            
            if response.status_code != 200:
                break
            
            soup = BeautifulSoup(response.text, 'html.parser')
            books = soup.find_all('article', class_='product_pod')
            
            if not books:
                break
            
            for book in books:
                book_data = extract_book_info(book, "https://books.toscrape.com")
                book_data['category'] = category_name
                all_books.append(book_data)
            
            page += 1
            time.sleep(0.5)

        except Exception as e:
            print(f"L·ªói: {e}")
            break

    # L∆∞u category
    if all_books:
        filename = f"books_{category_name.lower().replace(' ', '_')}.csv"
        try:
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                fieldnames = ['title', 'price', 'stock', 'rating', 'url', 'image_url', 'category']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                for book in all_books:
                    writer.writerow(book)
            print(f"ƒê√£ l∆∞u {len(all_books)} s√°ch v√†o {filename}")
        except Exception as e:
            print(f"L·ªói khi l∆∞u: {e}")

if __name__ == "__main__":
    print("Ch·ªçn mode scrape:")
    print("1. Scrape t·∫•t c·∫£ s√°ch (t·∫•t c·∫£ trang)")
    print("2. Scrape theo category")
    
    choice = input("Nh·∫≠p l·ª±a ch·ªçn (1 ho·∫∑c 2): ").strip()
    
    if choice == "1":
        scrape_books_toscrape()
    elif choice == "2":
        scrape_by_category()
    else:
        print("L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!") 